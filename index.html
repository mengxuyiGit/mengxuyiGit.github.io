<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xuyi Meng</title>

  <meta name="author" content="Xuyi Meng">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- <link rel="stylesheet" type="text/css" href="stylesheet.css"> -->
  <link rel="stylesheet" type="text/css" href="css/style.css?v=1.0">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🎋</text></svg>">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Xuyi Meng</name>
                  </p>
                  <p>
                    I am a 2nd-year CS M.S. student at <a href="https://www.upenn.edu/">University of Pennsylvania</a>,
                    advised by <a href="https://lingjie0206.github.io/">Prof. Lingjie Liu</a>.
                    Before that, I have acquired my B.E. degree in CS with <strong>highest distinction</strong> from
                    <a href="https://www.ntu.edu.sg/">Nanyang Technological University</a>, Singapore,
                    where I had the great opportunity to work in <a href="<a href="https://www.mmlab-ntu.com/">MMLab@NTU</a>,
                    advised by <a href="https://www.mmlab-ntu.com/person/ccloy/">Prof. Chen Change Loy</a> and <a href="daibo.info">Prof. Bo Dai</a>.
                    During my undergrad, I also spent a great summer in 2022 visiting <a href="https://www.ucsd.edu/">UCSD</a> and working with <a href="https://cseweb.ucsd.edu/~haosu/">Prof. Hao Su</a>.

                   
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:mengxuyi@seas.upenn.edu">Email</a> &nbsp/&nbsp
                    <a href="data/MengXuyi_CV_2024_Dec2.pdf">CV</a> &nbsp/&nbsp
                    <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                    <!-- <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Google Scholar</a> -->
                    <!-- &nbsp/&nbsp -->
                    <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                    <a href="https://github.com/mengxuyiGit/">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">

                  <a href="images/portrait-yslan.JPG"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/xuyi-conert.jpeg" class="hoverZoomLink"></a>

                </td>
              </tr>
            </tbody>
          </table>


          <div class="institute-logos">
            <img src="images/icon-Penn.png" alt="Institute 1" style="height: 50px; width: auto;" />
            <img src="images/icon-NTU.png" alt="Institute 2" style="height: 130px; width: auto;" />
            <img src="images/icon-UCSD.png" alt="Institute 3" style="height: 90px; width: auto;" />
            <!-- Add more as needed -->
        </div>
        

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    <!-- My interests lie in the intersection of computer vision, computer graphics, and machine learning,
                    particularly in inverse graphics powered by neural rendering,
                    including 3D generative models, shape analysis and 3D avatar, etc. -->
                    <!-- My research direction so far has been focusing on <strong>3D content generation</strong>,
                    particularly harnessing different generative backbone to generate point based rendering or neural rendering.
                    I am also interested in <strong>4D generation and reconstruction</strong>, 
                    where I have had experience in reconstructing human avatar from video data and render in real-time.
                    My research goal is to teach the model to learn rich priors and physics laws in the world 
                    using high dimentional data (3D/4D) through differentiable rendering. -->

                    <!-- My research mainly focuses on <strong>high quality 3D generation</strong>,
                    aiming at learning from abundant 2D data and
                    unleashing the powerful prior of large foundation models 
                    through differentiable neural rendering. 
                    I am also interested in <strong>human generation and reconstruction</strong>,
                    with experience in human avatar reconstruction from video data that can be rendered in real-time,
                    as well as generating 3D-aware high fidelity huamn faces. 
                    My research objective is to develop models that follows consistency and physical laws 
                    in high-dimensional space (3D/4D) while 
                    learning
                    from the prior available in rich 2D data
                    through differentiable rendering. -->


                    My research focuses on <strong>high-quality 3D generation </strong>
                    by leveraging abundant 2D data and the powerful priors of large foundation models 
                    through differentiable neural rendering. 
                    I am also dedicated to <strong>detailed human generation and reconstruction</strong>, 
                    with experience in real-time renderable human avatar reconstruction from video 
                    and generating 3D-aware, high-fidelity human faces. 
                    My objective is to develop models 
                    that maintain consistency and adhere to physical laws in 3D/4D spaces 
                    while learning from rich 2D data 
                    through differentiable rendering.
                    
                  </p>
                  <p>
                  <font color="red"><strong>I am actively looking for a Ph.D. position in 2025 Fall!</strong></font>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

<!-- ! below section TODO, after new paper accepted. -->
<!-- 
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <p> [2023-06] Joined Google AR as a student researcher, working with <a href="https://www.zhangyinda.com/">Yinda Zhang</a> .</p>
                  <p> [2023-03] Joined Google AR as a student researcher, working with <a href="https://www.zhangyinda.com/">Yinda Zhang</a> .</p>
                </td>
              </tr>
            </tbody>
          </table> -->
          


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <!-- zero1toG (ICLR 2025) -->
              <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='dreamfusion_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/dreamfusion.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/Homepage_v4.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function dreamfusion_start() {
                      document.getElementById('dreamfusion_image').style.opacity = "1";
                    }

                    function dreamfusion_stop() {
                      document.getElementById('dreamfusion_image').style.opacity = "0";
                    }
                    dreamfusion_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://dreamfusion3d.github.io/">
                    <papertitle>Zero-1-to-G: Direct 3D Generation using 2D Diffusion</papertitle>
                  </a>
                  <br>
                  <strong>Xuyi Meng</strong>,
                  <a href="https://cwchenwang.github.io/" target="_blank">Chen Wang</a>, 
                  <a href="https://www.cis.upenn.edu/~leijh/" target="_blank">Jiahui Lei</a>, 
                  <a href="https://www.cis.upenn.edu/~kostas/" target="_blank">Kostas Daniilidis</a>,
                  <a href="https://jiataogu.me/" target="_blank">Jiatao Gu</a>,
                  <a href="https://lingjie0206.github.io/" target="_blank">Lingjie Liu</a>
                  <em style="font-weight:bold; font-style:italic; margin-top:15px; margin-bottom:0px; display:block;">
                    arXiv 2024
                  </em>
                  <a href="https://zero1toG.github.io/">project page</a>
                  /
                  <a href="">arXiv</a>
                  /
                  <a href="">Github (code coming soon)</a>
                  <p>
                    We reframe the challenging task of direct 3D generation within a 2D diffusion framework, 
                    by decomposing the splatter image into a set of attribute images.
                    The use of pretrained 2D diffusion model enables high-quality 3D generation and good generalization ability to in-the-wild data.
                  </p>
                </td>
              </tr>

            
              <!-- ln3diff -->

              <tr onmouseout="e3dge_stop()" onmouseover="e3dge_start()">
                <!-- <tr> -->
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='e3dge_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/ln3diff/mast.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <!-- <img src='images/gaussian3diff/small_teaser.png' width="160"> -->
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://nirvanalan.github.io/projects/ln3diff/">
                    <papertitle>LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation
                    </papertitle>
                  </a>
                  <br>

                  <a href="https://nirvanalan.github.io/">Yushi Lan</a>,
                  <a href="https://hongfz16.github.io">Fangzhou Hong</a>,
                  <a href="https://williamyang1991.github.io/">Shuai Yang</a>,
                  <a href="https://shangchenzhou.com/">Shangchen Zhou</a>,
                  <strong>Xuyi Meng</strong>,
                  <!-- <a href="https://sg.linkedin.com/in/xuyi-meng-673779208"></a>, -->
                  <a href="https://daibo.info/">Bo Dai</a>,
                  <a href="https://xingangpan.github.io/">Xingang Pan</a>,
                  <a href="https://www.mmlab-ntu.com/person/ccloy/">Chen Change Loy</a>
                  <em style="font-weight:bold; font-style:italic; margin-top:15px; margin-bottom:0px; display:block;">
                    ECCV 2024
                  </em>
                  <a href="https://nirvanalan.github.io/projects/ln3diff/">project page</a>
                  /
                  <a href="https://arxiv.org/pdf/2403.12019.pdf">arXiv</a>
                  /
                  <a href="https://github.com/NIRVANALAN/LN3Diff">Github</a>
                  <img src="https://img.shields.io/github/stars/NIRVANALAN/LN3Diff?style=social" alt="GitHub stars">

                  <p></p>
                  <p>
                    LN3Diff creates high-quality 3D object mesh from text within 8 SECONDS. 
                  </p>
                </td>
              </tr>

      </tr>

      <tr onmouseout="e3dge_stop()" onmouseover="e3dge_start()">
        <!-- <tr> -->
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <!-- <div class="two" id='e3dge_image'><video width=100% height=100% muted autoplay loop>
                <source src="images/dreamfusion.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video></div> -->
            <img src='images/E3DGE/e3dge.png' width="160">
          </div>
          <!-- <script type="text/javascript">
            function e3dge_start() {
              document.getElementById('e3dge_image').style.opacity = "1";
            }

            function e3dge_stop() {
              document.getElementById('e3dge_image').style.opacity = "0";
            }
            e3dge_stop()
          </script> -->
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://nirvanalan.github.io/projects/E3DGE/index.html">
            <papertitle>E3DGE: Self-Supervised Geometry-Aware Encoder for Style-Based 3D GAN Inversion
            </papertitle>
          </a>
          <br>

          <a href="https://nirvanalan.github.io/">Yushi Lan</a>,
          <strong>Xuyi Meng</strong>,
          <a href='https://williamyang1991.github.io/' target='_blank'>Shuai Yang</a>,
          <a href='https://www.mmlab-ntu.com/person/ccloy/' target='_blank'>Chen Change Loy</a>,
          <a href='https://daibo.info/' target='_blank'>Bo Dai</a>
          <em style="font-weight:bold; font-style:italic; margin-top:15px; margin-bottom:0px; display:block;">
            CVPR 2023
          </em>
          <a href="https://nirvanalan.github.io/projects/E3DGE/index.html">project page</a>
          /
          <a href="https://arxiv.org/abs/2212.07409">arXiv</a>
          /
          <a href="https://drive.google.com/file/d/1yDkJfJOLeVlON7ZdRSnR34Ra_ikTVI0A/preview">video</a>
          /
          <a href="https://github.com/NIRVANALAN/CVPR23-E3DGE">Github</a>
          <img src="https://img.shields.io/github/stars/NIRVANALAN/CVPR23-E3DGE?style=social" alt="GitHub stars">


          <p></p>
          <p>
            We propose E3DGE, an encoder-based 3D GAN inversion framework that yields high-quality shape and
            texture reconstruction.
          </p>
        </td>
      </tr>




      <!-- template -->
      <!-- <tr onmouseout="dreamfusion_stop()" onmouseover="dreamfusion_start()" bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='dreamfusion_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/dreamfusion.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/dreamfusion.jpg' width="160">
                  </div>
                  <script type="text/javascript">
                    function dreamfusion_start() {
                      document.getElementById('dreamfusion_image').style.opacity = "1";
                    }

                    function dreamfusion_stop() {
                      document.getElementById('dreamfusion_image').style.opacity = "0";
                    }
                    dreamfusion_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://dreamfusion3d.github.io/">
                    <papertitle>DreamFusion: Text-to-3D using 2D Diffusion</papertitle>
                  </a>
                  <br>
                  <a href="https://cs.stanford.edu/~poole/">Ben Poole</a>,
                  <a href="https://www.ajayj.com/">Ajay Jain</a>,
                  <strong>Jonathan T. Barron</strong>,
                  <a href="https://bmild.github.io/">Ben Mildenhall</a>
                  <br>
                  <em>arXiv</em>, 2022
                  <br>
                  <a href="https://dreamfusion3d.github.io/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
                  /
                  <a href="https://dreamfusion3d.github.io/gallery.html">gallery</a>
                  <p></p>
                  <p>
                    We optimize a NeRF from scratch using a pretrained text-to-image diffusion model to do text-to-3D
                    generative modeling.
                  </p>
                </td>
              </tr>
 -->

    </tbody>
  </table>

  <!-- 
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody>
      <tr>
        <td>
          <heading>Misc</heading>
        </td>
      </tr>
    </tbody>
  </table> -->
  <table width="100%" align="center" border="0" cellpadding="20">
    <!-- <tbody> -->

    <!-- <tr>
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
                <td width="75%" valign="center">
                  <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                  <br>
                  <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member,
                    CVPR 2021</a>
                  <br>
                  <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                  <br>
                  <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
                </td> -->
    <!-- </tr> -->
    <!-- <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/cs188.jpg" alt="cs188">
                </td>
                <td width="75%" valign="center">
                  <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor,
                    CS188 Spring 2011</a>
                  <br>
                  <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor,
                    CS188 Fall 2010</a>
                  <br>
                  <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd
                    Edition</a>
                </td>
              </tr> -->


    <!-- <tr>
                <td align="center" style="padding:20px;width:25%;vertical-align:middle">
                  <heading>Basically <br> Blog Posts</heading>
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                  <br>
                  <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain
                    Functions</a>
                  <br>
                  <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                </td>
              </tr> -->


    </tbody>
  </table>


  <heading>Teaching</heading>
  <p> 
  TA for <a href="https://neural-representation-2024.github.io/">CIS7000: Neural Scene Representation and Neural Rendering </a>, 2024 Fall
  </p>

  <heading>Misc</heading>
  <p>
    🎹 I play <strong>2</strong> musical instruments: Piano and <a href="https://en.wikipedia.org/wiki/Guzheng">Guzheng</a>.<br>
    💃 I am skilled in <strong>3</strong> forms of dance: traditional Chinese dance, Latin dance, and street dance.<br>
    🌎 I can speak <strong>4</strong> languages: English, Mandarin, French, and Malay.<br>
    🧗‍♀️ I enjoy <strong>N</strong> types of exercise: <strong>gradient-ascending&descending</strong> (hiking, cycling), <strong>motion planning</strong> (bouldering), 
      <strong>human-object-interaction</strong> (weightlifting, badminton),
      <strong>simulation</strong> (swimming), pilates and yoga.
  </p>
  

  <table style="width:70%;border:0px;border-spacing:0px;">
  <!-- <table > -->
    <tbody>
      <tr>
        <td style="padding:0px">
          <br>
          <p style="text-align:right;font-size:small;">
            Thanks for the amazing source code from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's
              website 🧚 </a>
          </p>
        </td>
      </tr>
    </tbody>
  </table>
  </td>
  </tr>
  </table>
</body>

</html>
